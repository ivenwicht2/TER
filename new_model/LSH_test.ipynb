{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importation import extract\n",
    "from LocalitySensitiveHashing import *\n",
    "import numpy as np\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from datasketch import MinHashLSHEnsemble, MinHash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image,label,classe = extract(\"DATA\")\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classe) # Affiche les classes présentent dans le dossier\n",
    "print(label[0]) # Correspond au label de la première image. La place du 1 correspond au label, ici : D4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réccupération des images de chaque classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D21=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img,lb in zip(image,label):\n",
    "    if classe[np.argmax(lb)]==\"D21\" : #argmax renvoi dans la liste l'élément le plus gros \n",
    "        D21.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D4=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img,lb in zip(image,label):\n",
    "    if classe[np.argmax(lb)]==\"D4\" : #argmax renvoi dans la liste l'élément le plus gros *\n",
    "        D4.append(img)\n",
    "\n",
    "# * Puisque le nom de la classe est sous forme [0,0,1] at que la position du 1 correspond à la classe, le maximum de\n",
    "# la liste nous donne la classe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition des classes Hashtable et LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashTable:\n",
    "    def __init__(self, hash_size, inp_dimensions):\n",
    "        self.hash_size = hash_size\n",
    "        self.inp_dimensions = inp_dimensions\n",
    "        self.hash_table = dict()\n",
    "        self.projections = np.random.randn(self.hash_size, inp_dimensions)\n",
    "        \n",
    "    def generate_hash(self, inp_vector):\n",
    "        bools = (np.dot(inp_vector, self.projections.T) > 0).astype('int')\n",
    "        return ''.join(bools.astype('str'))\n",
    "\n",
    "    def __setitem__(self, inp_vec, label):\n",
    "        hash_value = self.generate_hash(inp_vec)\n",
    "        self.hash_table[hash_value] = self.hash_table\\\n",
    "            .get(hash_value, list()) + [label]\n",
    "        \n",
    "    def __getitem__(self, inp_vec):\n",
    "        hash_value = self.generate_hash(inp_vec)\n",
    "        return self.hash_table.get(hash_value, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=HashTable(224,190)\n",
    "test.generate_hash(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH:\n",
    "    def __init__(self, num_tables, hash_size, inp_dimensions):\n",
    "        self.num_tables = num_tables\n",
    "        self.hash_size = hash_size\n",
    "        self.inp_dimensions = inp_dimensions\n",
    "        self.hash_tables = list()\n",
    "        for i in range(self.num_tables):\n",
    "            self.hash_tables.append(HashTable(self.hash_size, self.inp_dimensions))\n",
    "    \n",
    "    def __setitem__(self, inp_vec, label):\n",
    "        for table in self.hash_tables:\n",
    "            table[inp_vec] = label\n",
    "    \n",
    "    def __getitem__(self, inp_vec):\n",
    "        results = list()\n",
    "        for table in self.hash_tables:\n",
    "            results.extend(table[inp_vec])\n",
    "        return list(set(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de la création d'une classe LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import des librairies\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras import applications\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import optimizers\n",
    "from scipy import spatial\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 105 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Import des images en 240 par 240 et noir et blanc\n",
    "data_dir = pathlib.Path(\"DATA\")\n",
    "image_count = len(list(data_dir.glob('*/*')))\n",
    "CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"]) \n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255) # redimension des images\n",
    "BATCH_SIZE = image_count\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "STEPS_PER_EPOCH = np.ceil(image_count/BATCH_SIZE)\n",
    "train_data_gen = image_generator.flow_from_directory(directory=str(data_dir),\n",
    "                                                     batch_size=BATCH_SIZE,\n",
    "                                                     shuffle=True,\n",
    "                                                     color_mode='grayscale', # Transformation des images en noir et blanc\n",
    "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                     classes = list(CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 224, 224, 1)\n"
     ]
    }
   ],
   "source": [
    "# Séparation images / labels\n",
    "image_batch, label_batch = next(train_data_gen)\n",
    "print(image_batch.shape)# Les données sont sur quatres dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7725491 , 0.7725491 , 0.6627451 , ..., 0.5803922 , 0.58431375,\n",
       "        0.58431375]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=image_batch.reshape(105,224*224) # On les veut sur une dimension : On a 988 images qui \n",
    "# correspondent chacunes à un vecteur de 50176 pixels\n",
    "data.shape\n",
    "\n",
    "don=np.array(data).reshape(1, -1)\n",
    "don"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "\n",
    "class LSH:\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.model = None\n",
    "\n",
    "    def __generate_random_vectors(self, num_vector, dim):\n",
    "        return np.random.randn(dim, num_vector)\n",
    "\n",
    "    def train(self, num_vector, seed=None):\n",
    "        dim = self.data.shape[1]\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        random_vectors = self.__generate_random_vectors(num_vector, dim)\n",
    "        powers_of_two = 1 << np.arange(num_vector - 1, -1, -1)\n",
    "\n",
    "        table = {}\n",
    "\n",
    "        # Partition data points into bins\n",
    "        bin_index_bits = (self.data.dot(random_vectors) >= 0)\n",
    "\n",
    "        # Encode bin index bits into integers\n",
    "        bin_indices = bin_index_bits.dot(powers_of_two)\n",
    "\n",
    "        # Update `table` so that `table[i]` is the list of document ids with bin index equal to i.\n",
    "        for data_index, bin_index in enumerate(bin_indices):\n",
    "            if bin_index not in table:\n",
    "                # If no list yet exists for this bin, assign the bin an empty list.\n",
    "                table[bin_index] = []\n",
    "            # Fetch the list of document ids associated with the bin and add the document id to the end.\n",
    "            table[bin_index].append(data_index)\n",
    "\n",
    "        self.model = {'bin_indices': bin_indices, 'table': table,\n",
    "                      'random_vectors': random_vectors, 'num_vector': num_vector}\n",
    "        return self\n",
    "\n",
    "    def __search_nearby_bins(self, query_bin_bits, table, search_radius=2, initial_candidates=set()):\n",
    "        num_vector = self.model['num_vector']\n",
    "        powers_of_two = 1 << np.arange(num_vector - 1, -1, -1)\n",
    "\n",
    "        # Allow the user to provide an initial set of candidates.\n",
    "        candidate_set = copy(initial_candidates)\n",
    "\n",
    "        for different_bits in combinations(range(num_vector), search_radius):\n",
    "            alternate_bits = copy(query_bin_bits)\n",
    "            for i in different_bits:\n",
    "                alternate_bits[i] = 1 if alternate_bits[i] == 0 else 0\n",
    "\n",
    "            # Convert the new bit vector to an integer index\n",
    "            nearby_bin = alternate_bits.dot(powers_of_two)\n",
    "\n",
    "            # Fetch the list of documents belonging to the bin indexed by the new bit vector.\n",
    "            # Then add those documents to candidate_set\n",
    "            if nearby_bin in table:\n",
    "                candidate_set.update(table[nearby_bin])\n",
    "\n",
    "        return candidate_set\n",
    "\n",
    "    def query(self, query_vec, k, max_search_radius, initial_candidates=set()):\n",
    "\n",
    "        if not self.model:\n",
    "            print('Model not yet build. Exiting!')\n",
    "            exit(-1)\n",
    "\n",
    "        data = self.data\n",
    "        table = self.model['table']\n",
    "        random_vectors = self.model['random_vectors']\n",
    "\n",
    "        bin_index_bits = (query_vec.dot(random_vectors) >= 0).flatten()\n",
    "\n",
    "        candidate_set = set()\n",
    "        # Search nearby bins and collect candidates\n",
    "        for search_radius in xrange(max_search_radius + 1):\n",
    "            candidate_set = self.__search_nearby_bins(bin_index_bits, table,\n",
    "                                                      search_radius, initial_candidates=initial_candidates)\n",
    "        # Sort candidates by their true distances from the query\n",
    "        nearest_neighbors = DataFrame({'id': list(candidate_set)})\n",
    "        candidates = data[np.array(list(candidate_set)), :]\n",
    "        nearest_neighbors['distance'] = pairwise_distances(candidates, query_vec, metric='cosine').flatten()\n",
    "        \n",
    "        return nearest_neighbors.nsmallest(k, 'distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-48a37dcd6360>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#find the 5 nearest neighbors of data[1] while searching in 10 buckets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlsh_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdon\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "#assumes that data is a num_observations by num_features numpy matrix\n",
    "lsh_model = LSH(don)\n",
    "num_of_random_vectors = 105\n",
    "lsh_model.train(num_of_random_vectors)\n",
    "xrange=range\n",
    "#find the 5 nearest neighbors of data[1] while searching in 10 buckets \n",
    "lsh_model.query(don[1,:], 5, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
